# Decision-Tree-On-Iris-Dataset



Dataset Exploration:
Embarked on a journey with the Iris dataset, immersing myself in the intricate world of decision trees. I split the data based on a preference column, optimizing for less entropy and higher information gain. Entropy, representing impurity, played a pivotal role in minimizing uncertainty in the decision-making process.

Entropy vs. Gini Impurity:
Explored both entropy and Gini impurity as measures of impurity. While Gini is computationally faster, I found that entropy was the preferred choice to mitigate overfitting - a significant challenge in decision tree models.

Dealing with Overfitting:
Overfitting can indeed pose a hurdle! I introduced various hyperparameters like maximum depth and criterion (using Gini or entropy) to fine-tune the decision tree and strike a balance between complexity and generalization.

GridSearchCV Magic:
Applied the magic of GridSearchCV to tackle overfitting in another dataset, achieving an impressive accuracy of 91.8%! GridSearchCV intelligently explores hyperparameter combinations, optimizing the decision tree's performance.

Key Takeaways:

Carefully chosen hyperparameters are vital for achieving optimal results.
GridSearchCV emerges as a powerful ally in the quest for improved decision tree performance.
Stay tuned for more insights and updates from my journey into the fascinating world of decision trees! 





Message ChatGPTâ€¦

C
